{"cells":[{"cell_type":"markdown","source":["# **Advancing Author-Specific Language Generation with a Custom Generative Pre-trained Transformer**\n","\n","#### Authors: Jingjia Meng, Emilia Liu\n","#### May 7th, 2024"],"metadata":{"id":"1jAM8ue1yDKk"},"id":"1jAM8ue1yDKk"},{"cell_type":"markdown","source":["# Environment Setup"],"metadata":{"id":"IupiB8pax3lK"},"id":"IupiB8pax3lK"},{"cell_type":"markdown","source":["## Defining Hyperparameters and Configurations"],"metadata":{"id":"bRPYlj3szWd4"},"id":"bRPYlj3szWd4"},{"cell_type":"code","execution_count":null,"id":"9abce638","metadata":{"id":"9abce638"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# Hyperparameters\n","batch_size = 64  # Number of sequences processed in parallel\n","block_size = 128  # Maximum sequence length for prediction\n","max_iters = 5000  # Maximum number of training iterations\n","eval_interval = 500  # Interval for evaluation during training\n","learning_rate = 3e-4  # Learning rate for the optimizer\n","\n","# Device configuration\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Model configuration\n","eval_iters = 200  # Number of batches for each evaluation\n","d_model = 384  # Dimensionality of the model's embeddings\n","n_head = 6  # Number of heads in the multi-head attention mechanism\n","n_layer = 6  # Number of sequential transformer blocks (decoder blocks)\n","dropout = 0.2  # Dropout rate\n","\n","# Seed for reproducibility\n","torch.manual_seed(888)"]},{"cell_type":"code","execution_count":null,"id":"YfvswJO5pAdS","metadata":{"id":"YfvswJO5pAdS"},"outputs":[],"source":["print(\"Using device:\", device)"]},{"cell_type":"markdown","source":["## Accessing Text Dataset"],"metadata":{"id":"xkGyTOqQzkzv"},"id":"xkGyTOqQzkzv"},{"cell_type":"code","execution_count":null,"id":"jlZjtYN2DBDs","metadata":{"id":"jlZjtYN2DBDs"},"outputs":[],"source":["# Download Hemingway text training data with public access\n","!gdown 'https://drive.google.com/uc?id=1f2mV6FuaruKLodrONRYBLtWkna_Y0Iay'"]},{"cell_type":"markdown","source":["## Data Cleaning and Preprocessing"],"metadata":{"id":"ZJKsjW1ezwdw"},"id":"ZJKsjW1ezwdw"},{"cell_type":"code","execution_count":null,"id":"1930b1d3","metadata":{"id":"1930b1d3"},"outputs":[],"source":["# Character level text processing and data preparation\n","import re\n","\n","# Read the file, delete \\n, format spacing and return the text\n","def read_text(file_name):\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","    text = text.replace('\\n', ' ')\n","    text = re.sub(' +', ' ', text)\n","    text = re.sub(r'\\s+([.,;:])', r'\\1', text)\n","    return text\n","\n","# Create mappings from characters to integers and vice versa\n","def create_mappings(text):\n","    characters = sorted(set(text))\n","    stoi = {char: idx for idx, char in enumerate(characters)}\n","    itos = {idx: char for idx, char in enumerate(characters)}\n","    vocab_size = len(characters)  # Calculate the vocabulary size\n","    return stoi, itos, vocab_size\n","\n","# Reading and processing the text\n","text = read_text('hemingway_new.txt')\n","stoi, itos, vocab_size = create_mappings(text)\n","\n","# Encode and decode lambda functions\n","encode = lambda s: [stoi[c] for c in s]  # Encoder\n","decode = lambda l: ''.join([itos[i] for i in l])  # Decoder\n","\n","# Creating the data tensor\n","data = torch.tensor(encode(text), dtype=torch.long)\n","\n","# Splitting the data into training and validation sets\n","n = int(0.9 * len(data))\n","train_data, val_data = data[:n], data[n:]"]},{"cell_type":"markdown","source":["# Model Training"],"metadata":{"id":"RI-GWnud0GOc"},"id":"RI-GWnud0GOc"},{"cell_type":"markdown","source":["## Data Batching"],"metadata":{"id":"SNbVasR81czX"},"id":"SNbVasR81czX"},{"cell_type":"code","execution_count":null,"id":"bade6f32","metadata":{"id":"bade6f32"},"outputs":[],"source":["import numpy as np\n","\n","def get_batch(data_partition):\n","    \"\"\"\n","    Fetch a mini-batch of data containing inputs (x) and targets (y).\n","    The data_partition argument determines whether to fetch from training or validation data.\n","    \"\"\"\n","\n","    # Choose the data based on the provided partition: training or validation\n","    data = train_data if data_partition == 'train' else val_data\n","    # Random selection of indices for batch_size data points\n","    indices = np.random.choice(len(data) - block_size, size=batch_size, replace=False)\n","    # Gather input data chunks of block_size length at selected indices\n","    x_batch = [data[idx:idx + block_size] for idx in indices]\n","    # Gather target data, which is offset by one from the inputs\n","    y_batch = [data[idx + 1:idx + 1 + block_size] for idx in indices]\n","    # Stack the input and target batches and move them to the specified device\n","    x_batch, y_batch = torch.stack(x_batch).to(device), torch.stack(y_batch).to(device)\n","    # The shape of x_batch and y_batch is (batch_size, block_size)\n","    return x_batch, y_batch"]},{"cell_type":"markdown","source":["## Loss Evaluation Across Data Sets"],"metadata":{"id":"DvBeH4eb1fN3"},"id":"DvBeH4eb1fN3"},{"cell_type":"code","execution_count":null,"id":"938086a6","metadata":{"id":"938086a6"},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss(model):\n","    \"\"\"\n","    Evaluate the model's loss on both training and validation sets without gradient updates.\n","    The function calculates the mean loss for each set and returns these values.\n","    \"\"\"\n","    results = {}\n","    # Evaluate model performance in a non-training context\n","    model.eval()\n","\n","    # Loop over both training and validation data sets\n","    for partition in ['train', 'val']:\n","        # Initialize a tensor to store losses for each iteration\n","        loss_values = torch.zeros(eval_iters, dtype=torch.float32, device=device)\n","        for i in range(eval_iters):\n","            # Obtain a batch of data\n","            x_batch, y_batch = get_batch(partition)\n","            # Compute the model's output and loss for the batch\n","            logits, batch_loss = model(x_batch, y_batch)\n","            # Store the computed loss\n","            loss_values[i] = batch_loss\n","        # Compute and store the mean loss for the current partition\n","        results[partition] = loss_values.mean()\n","    # Revert model to training mode\n","    model.train()\n","\n","    return results"]},{"cell_type":"markdown","source":["## Multi-Head Attention Implementation"],"metadata":{"id":"nH_qP0ac2M86"},"id":"nH_qP0ac2M86"},{"cell_type":"code","execution_count":null,"id":"10e26176","metadata":{"id":"10e26176"},"outputs":[],"source":["class SelfAttentionHead(nn.Module):\n","    \"\"\"\n","    Implements a single head of self-attention mechanism, particularly for a decoder\n","    utilizing masked self-attention to prevent future information leakage.\n","    \"\"\"\n","\n","    def __init__(self, d_head):\n","        super().__init__()\n","        # Initialize the dimensions for keys, queries, and values\n","        self.d_head = d_head\n","        self.key_transform = nn.Linear(d_model, d_head, bias=False)\n","        self.query_transform = nn.Linear(d_model, d_head, bias=False)\n","        self.value_transform = nn.Linear(d_model, d_head, bias=False)\n","\n","        # Lower triangular matrix to apply masked attention\n","        self.register_buffer('mask', torch.tril(torch.ones(block_size, block_size)))\n","\n","        # Dropout layer for attention weights\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, _ = x.shape\n","\n","        # Generate keys, queries, and values for self-attention\n","        keys = self.key_transform(x)\n","        queries = self.query_transform(x)\n","        values = self.value_transform(x)\n","\n","        # Compute the dot product attention weights\n","        attention_scores = torch.matmul(queries, keys.transpose(1, 2)) / (self.d_head ** 0.5)\n","\n","        # Apply masking to ensure causality in decoder\n","        attention_scores = attention_scores.masked_fill(self.mask[:seq_length, :seq_length] == 0, float('-inf'))\n","\n","        # Normalize attention scores and apply dropout\n","        attention_weights = self.dropout(F.softmax(attention_scores, dim=-1))\n","\n","        # Compute the weighted sum of values\n","        attention_output = torch.matmul(attention_weights, values)\n","        return attention_output\n","\n","class ParallelMultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Aggregates multiple self-attention heads, each computing a distinct aspect of the input,\n","    and combines their outputs.\n","    \"\"\"\n","\n","    def __init__(self, num_heads, d_head):\n","        super().__init__()\n","        # Initialize multiple attention heads\n","        self.heads = nn.ModuleList([SelfAttentionHead(d_head) for _ in range(num_heads)])\n","        # Linear layer to project the concatenated outputs back to the original dimension\n","        self.output_transform = nn.Linear(num_heads * d_head, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # Concatenate outputs from all attention heads\n","        multi_head_output = torch.cat([head(x) for head in self.heads], dim=-1)\n","        # Project the concatenated output to d_model dimension and apply dropout\n","        return self.dropout(self.output_transform(multi_head_output))"]},{"cell_type":"markdown","source":["## Feedforward Layer Implementation"],"metadata":{"id":"Smz-ZSJl2bPh"},"id":"Smz-ZSJl2bPh"},{"cell_type":"code","execution_count":null,"id":"XdEtmrPm7ZCD","metadata":{"id":"XdEtmrPm7ZCD"},"outputs":[],"source":["class FeedForwardLayer(nn.Module):\n","    \"\"\"\n","    Defines a feedforward neural network layer applied at the token level.\n","    It consists of two linear transformations with a ReLU activation in between.\n","    \"\"\"\n","\n","    def __init__(self, d_model):\n","        super(FeedForwardLayer, self).__init__()\n","        # Define the dimensionality of the feedforward network, typically 4 times d_model\n","        d_ff = 4 * d_model\n","        # Initialize the feedforward network components\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        The forward pass of the feedforward layer.\n","\n","        :param x: The input tensor.\n","        :return: The output of the feedforward layer.\n","        \"\"\"\n","        x = self.linear1(x)  # First linear transformation\n","        x = self.relu(x)     # Apply ReLU non-linearity\n","        x = self.linear2(x)  # Second linear transformation\n","        x = self.dropout(x)  # Apply dropout\n","        return x"]},{"cell_type":"markdown","source":["## Transformer Decoder Block Implementation"],"metadata":{"id":"CAUAmPiJ2smH"},"id":"CAUAmPiJ2smH"},{"cell_type":"code","execution_count":null,"id":"ddb29049","metadata":{"id":"ddb29049"},"outputs":[],"source":["class TransformerDecoderBlock(nn.Module):\n","    \"\"\"\n","    Represents a single block within a transformer's decoder architecture,\n","    comprising a multi-head self-attention mechanism and a position-wise feedforward network.\n","    Each block applies these components in sequence, followed by layer normalization.\n","    \"\"\"\n","\n","    def __init__(self, d_model, n_head):\n","        super(TransformerDecoderBlock, self).__init__()\n","        # Determine the size of each attention head's representation\n","        d_head = d_model // n_head\n","        # Initialize the multi-head self-attention component\n","        self.multihead_attention = ParallelMultiHeadAttention(n_head, d_head)\n","        # Initialize the feedforward network component\n","        self.feedforward = FeedForwardLayer(d_model)\n","        # Layer normalization components after the self-attention and feedforward network\n","        self.norm_layer1 = nn.LayerNorm(d_model)\n","        self.norm_layer2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Processes input through one decoder block, including self-attention and feedforward network.\n","\n","        :param x: Input tensor to the decoder block.\n","        :return: Output tensor of the decoder block.\n","        \"\"\"\n","        # Apply multi-head self-attention followed by layer normalization with residual connection\n","        attention_output = self.multihead_attention(x)\n","        x = self.norm_layer1(x + attention_output)\n","        # Apply the feedforward network followed by layer normalization with residual connection\n","        feedforward_output = self.feedforward(x)\n","        x = self.norm_layer2(x + feedforward_output)\n","        return x"]},{"cell_type":"markdown","source":["## Token Generation in Simplified GPT"],"metadata":{"id":"MT4pMKd23ZDE"},"id":"MT4pMKd23ZDE"},{"cell_type":"code","execution_count":null,"id":"5c230527","metadata":{"id":"5c230527"},"outputs":[],"source":["class GPTModel(nn.Module):\n","    \"\"\"\n","    A simplified version of the GPT model, incorporating token and position embeddings,\n","    followed by a sequence of transformer decoder blocks, and concluding with a linear\n","    layer to produce logits for next token prediction.\n","    \"\"\"\n","    def __init__(self):\n","        super(GPTModel, self).__init__()\n","        # Initialize the embedding tables for tokens and positions\n","        self.token_embeddings = nn.Embedding(vocab_size, d_model)\n","        self.position_embeddings = nn.Embedding(block_size, d_model)\n","        # Sequentially stack decoder blocks to form the transformer's body\n","        self.decoder_blocks = nn.Sequential(*[TransformerDecoderBlock(d_model, n_head) for _ in range(n_layer)])\n","        # Final layer normalization\n","        self.final_norm = nn.LayerNorm(d_model)\n","        # Linear transformation to project the output to the vocabulary size\n","        self.output_linear = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape  # Batch size and sequence length\n","\n","        # Embed tokens and positions\n","        token_emb = self.token_embeddings(idx)\n","        position_emb = self.position_embeddings(torch.arange(T, device=idx.device))\n","\n","        # Combine token and position embeddings\n","        x = token_emb + position_emb.unsqueeze(0)\n","\n","        # Process the input through the decoder blocks\n","        x = self.decoder_blocks(x)\n","\n","        # Apply the final normalization and project to vocabulary size\n","        x = self.final_norm(x)\n","        logits = self.output_linear(x)\n","\n","        # Calculate loss if targets are provided\n","        loss = None\n","        if targets is not None:\n","            logits_flattened = logits.view(B * T, -1)\n","            targets_flattened = targets.view(B * T)\n","            loss = F.cross_entropy(logits_flattened, targets_flattened)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        \"\"\"\n","        Generates new tokens given a context.\n","\n","        :param idx: Tensor of shape (B, T) containing token indices.\n","        :param max_new_tokens: Maximum number of new tokens to generate.\n","        :return: Tensor of shape (B, T + max_new_tokens) with the original and new tokens.\n","        \"\"\"\n","        self.eval()  # Set the model to evaluation mode\n","        for _ in range(max_new_tokens):\n","            # Handle the context window limitation\n","            idx_cond = idx[:, -block_size:]\n","\n","            # Predict the next token\n","            logits, _ = self(idx_cond)\n","            logits = logits[:, -1, :]  # Focus on the logits for the last predicted token\n","            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n","\n","            # Sample the next token from the probability distribution\n","            next_token = torch.multinomial(probs, 1)\n","            idx = torch.cat([idx, next_token], dim=1)  # Append the new token to the sequence\n","\n","        self.train()  # Revert to training mode\n","        return idx"]},{"cell_type":"markdown","source":["## Early Stopping Mechanism"],"metadata":{"id":"bqFzxtQt3s7u"},"id":"bqFzxtQt3s7u"},{"cell_type":"code","execution_count":null,"id":"ThiIDDj1gWse","metadata":{"id":"ThiIDDj1gWse"},"outputs":[],"source":["class EarlyStoppingMechanism:\n","    \"\"\"\n","    Implements early stopping to halt training when validation loss ceases to decrease significantly.\n","    \"\"\"\n","\n","    def __init__(self, tolerance=5, min_delta=0):\n","        \"\"\"\n","        Initializes the early stopping parameters.\n","\n","        :param tolerance: The number of epochs with no significant improvement after which training will be stopped.\n","        :param min_delta: The minimum relative change in validation loss considered as an improvement.\n","        \"\"\"\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.counter = 0  # Counter for epochs without significant improvement\n","        self.early_stop = False  # Indicates whether to stop the training\n","\n","    def __call__(self, train_loss, validation_loss):\n","        \"\"\"\n","        Evaluates whether the training should stop early based on the recent trend in validation loss.\n","\n","        :param train_loss: The current training loss.\n","        :param validation_loss: The current validation loss.\n","        \"\"\"\n","        relative_improvement = (validation_loss - train_loss) / train_loss\n","\n","        # Check if the improvement is less than the minimum delta\n","        if relative_improvement > self.min_delta:\n","            # Increment the counter if there is no significant improvement\n","            self.counter += 1\n","            # Trigger early stopping if the counter exceeds the tolerance threshold\n","            if self.counter >= self.tolerance:\n","                self.early_stop = True\n","        else:\n","            # Reset counter if there is significant improvement\n","            self.counter = 0"]},{"cell_type":"markdown","source":["## Optimizing Model with Evaluation"],"metadata":{"id":"HI-Hovuj4GgZ"},"id":"HI-Hovuj4GgZ"},{"cell_type":"code","execution_count":null,"id":"sJu3FQkBqT_o","metadata":{"id":"sJu3FQkBqT_o"},"outputs":[],"source":["import torch.optim as optim\n","\n","# Initialize the GPT-like model and move it to the specified device\n","model = GPTModel().to(device)\n","\n","# Calculate and display the total number of trainable parameters in the model\n","num_params = sum(p.numel() for p in model.parameters())\n","print(f\"Number of parameters in the model: {num_params}\")\n","\n","# Set up the optimizer with the AdamW algorithm\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Define a learning rate scheduler\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","\n","# Initialize the early stopping mechanism\n","early_stopping = EarlyStoppingMechanism(tolerance=1, min_delta=0.2)\n","\n","# Training loop\n","for iteration in range(max_iters):\n","    # Periodically evaluate the model on training and validation sets\n","    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n","        # Adjust the learning rate based on the scheduler\n","        if iteration:\n","            scheduler.step()\n","\n","        # Estimate the loss on training and validation datasets\n","        evaluation_losses = estimate_loss(model)\n","        print(f\"Step {iteration}: Train Loss: {evaluation_losses['train']:.4f}, Validation Loss: {evaluation_losses['val']:.4f}\")\n","\n","        # Check for early stopping\n","        early_stopping(evaluation_losses['train'], evaluation_losses['val'])\n","        if early_stopping.early_stop:\n","            print(f\"Early stopping triggered at iteration {iteration}\")\n","            break\n","\n","    # Get a batch of training data\n","    xb, yb = get_batch('train')\n","\n","    # Forward pass to calculate the loss\n","    logits, loss = model(xb, yb)\n","\n","    # Backward pass to calculate gradients\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Update model parameters\n","    optimizer.step()"]},{"cell_type":"markdown","source":["## Text Generation and Saving Output"],"metadata":{"id":"qGGoIMwR4V04"},"id":"qGGoIMwR4V04"},{"cell_type":"code","execution_count":null,"id":"d6Evv_nT8u9X","metadata":{"id":"d6Evv_nT8u9X"},"outputs":[],"source":["# Initialize the generation context with an empty token\n","initial_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","\n","# Generate a sequence of up to 1000 tokens from the model starting with the initial context\n","generated_sequence_long = model.generate(initial_context, max_new_tokens=1000)\n","# Convert the generated token IDs to text\n","generated_text_long = decode(generated_sequence_long[0].tolist())\n","print(generated_text_long)\n","\n","# Generate a shorter sequence of up to 100 tokens for saving to a file\n","#generated_sequence_short = model.generate(initial_context, max_new_tokens=100)\n","# Convert the generated token IDs to text\n","#generated_text_short = decode(generated_sequence_short[0].tolist())\n","\n","# Save the shorter generated text to a file\n","with open('fake_hemingway.txt', 'w', encoding='utf-8') as file:\n","    file.write(generated_text_long)"]},{"cell_type":"code","execution_count":null,"id":"8163ae4f","metadata":{"id":"8163ae4f"},"outputs":[],"source":["torch.save(model.state_dict(), 'gpt.pt')"]},{"cell_type":"markdown","id":"4zhCp_0Obtgz","metadata":{"id":"4zhCp_0Obtgz"},"source":["# Model Evaluation Using Quantitative Metrics"]},{"cell_type":"code","execution_count":null,"id":"7kFFnL38-fzx","metadata":{"id":"7kFFnL38-fzx"},"outputs":[],"source":["!pip install rouge"]},{"cell_type":"code","execution_count":null,"id":"7UCbiaqZf--n","metadata":{"id":"7UCbiaqZf--n"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import sentence_bleu\n","from rouge import Rouge\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"markdown","id":"K5SPMCpXd4Wp","metadata":{"id":"K5SPMCpXd4Wp"},"source":["## Lexical Diversity (NLTK)"]},{"cell_type":"code","execution_count":null,"id":"W9Qr6x6QJ3Xg","metadata":{"id":"W9Qr6x6QJ3Xg"},"outputs":[],"source":["# Lexical Diversity using NLTK\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Function to calculate lexical diversity of a text\n","def lexical_diversity(text):\n","    tokens = word_tokenize(text)\n","    unique_words = set(tokens)\n","    return len(unique_words) / len(tokens)\n","\n","# Function to load text from a file\n","def load_text(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        return file.read()\n","\n","# Path to the text file\n","candidate_text_path = 'fake_hemingway.txt' # Model-generated Hemingway texts\n","\n","# Example text\n","reference_text = text\n","candidate_text = load_text(candidate_text_path)\n","\n","reference_diversity_score = lexical_diversity(reference_text)\n","candidate_text_diversity_score = lexical_diversity(candidate_text)\n","print(f\"Lexical Diversity (Training): {reference_diversity_score}\")\n","print(f\"Lexical Diversity (Generated): {candidate_text_diversity_score}\")"]},{"cell_type":"markdown","id":"4UwmMpVzd-IT","metadata":{"id":"4UwmMpVzd-IT"},"source":["## Sentence Complexity (SpaCy)"]},{"cell_type":"code","execution_count":null,"id":"sP3OprnjIj3V","metadata":{"id":"sP3OprnjIj3V"},"outputs":[],"source":["# Sentence Complexity using SpaCy\n","# Sentence complexity can be gauged by analyzing the average sentence length or the use of complex syntactic structures\n","\n","import spacy\n","\n","def average_sentence_length(text):\n","    nlp = spacy.load('en_core_web_sm')\n","    # Increase the max length of text that the nlp object will accept\n","    nlp.max_length = 1500000  # Adjust this number based on text length and available memory\n","    doc = nlp(text)\n","    sentences = list(doc.sents)\n","    total_length = sum(len(sentence) for sentence in sentences)\n","    return total_length / len(sentences)\n","\n","# Example text\n","reference_sentence_len = average_sentence_length(reference_text)\n","candidate_sentence_len = average_sentence_length(candidate_text)\n","print(f\"Average Sentence Length (Train): {reference_sentence_len}\")\n","print(f\"Average Sentence Length (Generated): {candidate_sentence_len}\")"]},{"cell_type":"markdown","id":"FioBH7Kae4Zh","metadata":{"id":"FioBH7Kae4Zh"},"source":["## Parts of Speech (POS) Usage Frequency"]},{"cell_type":"code","execution_count":null,"id":"6i5GNs36RuRs","metadata":{"id":"6i5GNs36RuRs"},"outputs":[],"source":["# POS frequency\n","\n","import spacy\n","\n","def pos_frequency(text):\n","    try:\n","        nlp = spacy.load('en_core_web_sm')\n","        nlp.max_length = len(text) + 100  # Small buffer to exceed the text length\n","        doc = nlp(text)\n","        pos_counts = doc.count_by(spacy.attrs.POS)\n","        return {doc.vocab[key].text: value for key, value in pos_counts.items()}\n","    except Exception as e:\n","        print(f\"Error processing text: {e}\")\n","        return {}\n","\n","# Get POS frequencies\n","reference_frequency = pos_frequency(reference_text)\n","candidate_frequency = pos_frequency(candidate_text)\n","\n","# Function to print POS counts\n","def print_pos_counts(pos_counts):\n","    for pos, count in sorted(pos_counts.items()):\n","        print(f\"{pos}: {count}\")\n","\n","# Print POS frequencies\n","print(\"Reference Text POS Frequencies:\")\n","print_pos_counts(reference_frequency)\n","print(\"\\nCandidate Text POS Frequencies:\")\n","print_pos_counts(candidate_frequency)"]},{"cell_type":"code","execution_count":null,"id":"bmSWZZYCP2W_","metadata":{"id":"bmSWZZYCP2W_"},"outputs":[],"source":["# Calculate frequency %\n","def print_pos_percentages(pos_counts):\n","    total_pos = sum(pos_counts.values())\n","    pos_percentages = {pos: (count / total_pos) * 100 for pos, count in pos_counts.items()}\n","    for pos, percentage in sorted(pos_percentages.items()):\n","        print(f\"{pos}: {percentage:.2f}%\")\n","\n","# Define POS counts for reference and candidate texts with POS tags in dictionaries\n","reference_counts = {\n","    \"ADJ\":11374,\"ADP\": 21258,\"ADV\": 11944,\"AUX\": 13706,\"CCONJ\": 8141,\n","    \"DET\": 21570,\"INTJ\": 1402,\"NOUN\": 32907,\"NUM\": 2818,\"PART\": 5752,\n","    \"PRON\": 32237,\"PROPN\": 9597,\"PUNCT\": 41303,\"SCONJ\": 4121,\"SPACE\": 1,\n","    \"SYM\": 146,\"VERB\": 33158,\"X\": 152\n","}\n","candidate_counts = {\n","    \"ADJ\":11,\"ADP\": 22,\"ADV\": 12,\"AUX\": 18,\"CCONJ\": 4,\n","    \"DET\": 23,\"INTJ\": 1,\"NOUN\": 29,\"NUM\": 3,\"PART\": 11,\n","    \"PRON\": 29,\"PROPN\": 12,\"PUNCT\": 40,\"SCONJ\": 2,\"SPACE\": 1,\n","    \"SYM\": 0,\"VERB\": 28,\"X\": 0\n","}\n","\n","# Print POS percentages for reference and candidate texts\n","print(\"Reference Text POS Percentages:\")\n","print_pos_percentages(reference_counts)\n","print(\"\\nCandidate Text POS Percentages:\")\n","print_pos_percentages(candidate_counts)"]},{"cell_type":"markdown","id":"HW7xWRt6JDpr","metadata":{"id":"HW7xWRt6JDpr"},"source":["### Visualization (Figure 1): Comparison of POS Frequency Distributions"]},{"cell_type":"code","execution_count":null,"id":"ccoW3ejqaruU","metadata":{"id":"ccoW3ejqaruU"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Sort keys to ensure alignment between reference and candidate counts\n","keys = sorted(reference_counts.keys())\n","\n","# Convert raw counts to percentages\n","total_ref = sum(reference_counts.values())\n","total_cand = sum(candidate_counts.values())\n","ref_percentages = [100 * reference_counts[k] / total_ref for k in keys]\n","cand_percentages = [100 * candidate_counts[k] / total_cand for k in keys]\n","\n","# Create side-by-side bar plot\n","fig, ax = plt.subplots()\n","index = range(len(keys))\n","bar_width = 0.35\n","\n","rects1 = ax.bar(index, ref_percentages, bar_width, label='Reference')\n","rects2 = ax.bar([p + bar_width for p in index], cand_percentages, bar_width, label='Generated')\n","\n","ax.set_xlabel('Parts of Speech')\n","ax.set_ylabel('Frequency (%)')\n","ax.set_title('Figure 1. Comparison of POS Frequency Percentages Between Reference and Candidate Texts')\n","ax.set_xticks([p + bar_width / 2 for p in index])\n","ax.set_xticklabels(keys)\n","ax.legend()\n","\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"5flokZ25dyaR","metadata":{"id":"5flokZ25dyaR"},"source":["## ROUGE Score (N-gram Overlap)"]},{"cell_type":"code","execution_count":null,"id":"AHr4rLYcdgcd","metadata":{"id":"AHr4rLYcdgcd"},"outputs":[],"source":["# Load reference and candidate texts for ROUGE evaluation\n","def load_text(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        return file.read()\n","\n","# reference_text_path = text # Actual Hemingway texts\n","candidate_text_path = 'fake_hemingway.txt' # Model-generated Hemingway texts\n","\n","reference_text = text\n","candidate_text = load_text(candidate_text_path)\n","\n","# Split the reference text into sentences\n","reference_sentences = nltk.sent_tokenize(reference_text.lower())\n","\n","# Choose a segment from the reference text that matches the candidate text length\n","# Function to calculate the start index for the reference text based on TF-IDF and cosine similarity\n","def find_most_similar_segment(reference_sentences, candidate_text, n_sentences):\n","    # Initialize the vectorizer\n","    vectorizer = TfidfVectorizer()\n","    # Create a corpus with the candidate text and all reference sentences\n","    corpus = [candidate_text] + reference_sentences\n","    # Fit and transform the corpus\n","    tfidf_matrix = vectorizer.fit_transform(corpus)\n","    # Calculate cosine similarity between the candidate text and all reference sentences\n","    similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n","    # Find the index of the highest similarity score\n","    best_match_index = np.argmax(similarities)\n","    # Calculate the start index to get n_sentences around the best match\n","    start_index = max(0, best_match_index - n_sentences // 2)\n","    end_index = start_index + n_sentences\n","    return start_index, end_index\n","\n","# Find the most similar segment of the reference text to the candidate text\n","n_sentences_to_compare = 100  # Can adjust this to the average number of sentences in 100 tokens\n","start_index, end_index = find_most_similar_segment(reference_sentences, candidate_text, n_sentences_to_compare)\n","\n","# Extract the segment from the reference text\n","reference_segment = \" \".join(reference_sentences[start_index:end_index])\n","\n","# Calculate ROUGE scores using the selected segment\n","# rouge_scores = calculate_rouge_score(reference_segment, candidate_text)\n","rouge_seg = Rouge()\n","rouge_score_seg = rouge_seg.get_scores(candidate_text, reference_segment)\n","\n","print(f\"Selected segment from index {start_index} to {end_index} for evaluation.\")\n","rouge_score_seg"]},{"cell_type":"code","execution_count":null,"id":"g6IyUdx9JlU8","metadata":{"id":"g6IyUdx9JlU8"},"outputs":[],"source":["# Compare: ROUGE scores without segmenting reference text (using full reference text)\n","rouge = Rouge()\n","rouge_score = rouge.get_scores(candidate_text, reference_text)\n","\n","print(f\"Using full reference text without segments for evaluation.\")\n","rouge_score"]},{"cell_type":"markdown","id":"6jSymVqvhLy5","metadata":{"id":"6jSymVqvhLy5"},"source":["## METEOR Score"]},{"cell_type":"code","execution_count":null,"id":"TGWeNxU1jien","metadata":{"id":"TGWeNxU1jien"},"outputs":[],"source":["from nltk.translate.meteor_score import meteor_score\n","from nltk.tokenize import word_tokenize\n","nltk.download('wordnet')\n","\n","# Tokenize the segmented reference text and wrap it in a list (instead of string)\n","reference_seg = [word_tokenize(reference_segment)]\n","\n","# Tokenize the candidate text and keep it as a string\n","candidate_text = load_text(candidate_text_path)\n","candidate_texts = word_tokenize(candidate_text)\n","\n","# Calculate METEOR score\n","meteor_result = meteor_score(reference_seg, candidate_texts)\n","\n","print(f\"Selected segment from index {start_index} to {end_index} for evaluation.\")\n","print(f\"METEOR Score: {meteor_result}\")"]},{"cell_type":"markdown","id":"49xzr6jcmpzk","metadata":{"id":"49xzr6jcmpzk"},"source":["## BERT Score"]},{"cell_type":"code","execution_count":null,"id":"jEwK6R2YnElG","metadata":{"id":"jEwK6R2YnElG"},"outputs":[],"source":["!pip install bert_score"]},{"cell_type":"code","execution_count":null,"id":"pUCbv3TumrPd","metadata":{"id":"pUCbv3TumrPd"},"outputs":[],"source":["from bert_score import score as bert_score\n","\n","def calculate_bert_score(references, candidates, lang='en'):\n","    \"\"\"\n","    Calculate BERTScore between lists of reference and candidate texts.\n","    \"\"\"\n","    P, R, F1 = bert_score(candidates, references, lang=lang, rescale_with_baseline=True)\n","    return P.mean().item(), R.mean().item(), F1.mean().item()\n","\n","# Calculate BERT score\n","P, R, F1 = calculate_bert_score([reference_segment], [candidate_text])\n","print(f\"BERTScore Precision: {P}, Recall: {R}, F1 Score: {F1}\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}